{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c38ee984",
   "metadata": {},
   "source": [
    "# 3D U-Net Training for Medical Image Segmentation\n",
    "\n",
    "This notebook implements a 3D U-Net for binary tumor segmentation using medical imaging data.\n",
    "\n",
    "## Configuration\n",
    "Set your data paths and training parameters below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8344680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "IMAGES_DIR = \"/home/naiken/coding/atlas/atlas-train-dataset-1.0.1/train/imagesTr\"\n",
    "LABELS_DIR = \"/home/naiken/coding/atlas/atlas-train-dataset-1.0.1/train/labelsTr\"\n",
    "EPOCHS = 10  # Reduced for testing\n",
    "BATCH_SIZE = 1  # Reduced for CPU training\n",
    "PATCH_SIZE = (64, 64, 32)  # Reduced for faster training\n",
    "VAL_SPLIT = 0.2\n",
    "LR = 1e-4\n",
    "SAVE_PATH = \"best_unet3d.pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fd7731",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87833dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version: 2.9.0+cu128\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from glob import glob\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "print(f\"Using PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cda0366",
   "metadata": {},
   "source": [
    "## Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1a7843d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NiftiSegDataset(Dataset):\n",
    "    def __init__(self, images: List[str], labels: List[str], patch_size: Tuple[int,int,int]=None, augment: bool=True):\n",
    "        assert len(images) == len(labels)\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.patch_size = patch_size\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = nib.load(self.images[idx]).get_fdata(dtype=np.float32)\n",
    "        lbl = nib.load(self.labels[idx]).get_fdata(dtype=np.float32)\n",
    "\n",
    "        lbl = (lbl == 2).astype(np.uint8)\n",
    "        img = (img - img.mean()) / (img.std() + 1e-7)\n",
    "\n",
    "        img = np.expand_dims(img, 0).astype(np.float32)\n",
    "        lbl = np.expand_dims(lbl, 0).astype(np.float32)\n",
    "\n",
    "        img_t = torch.from_numpy(img)\n",
    "        lbl_t = torch.from_numpy(lbl)\n",
    "\n",
    "        if self.patch_size is not None:\n",
    "            img_t, lbl_t = self.random_crop(img_t, lbl_t, self.patch_size)\n",
    "\n",
    "        if self.augment:\n",
    "            img_t, lbl_t = self.random_flip(img_t, lbl_t)\n",
    "\n",
    "        return img_t, lbl_t\n",
    "\n",
    "    @staticmethod\n",
    "    def random_crop(img: torch.Tensor, lbl: torch.Tensor, size: Tuple[int,int,int]):\n",
    "        _, D, H, W = img.shape\n",
    "        sd, sh, sw = size\n",
    "        \n",
    "        # Always pad to at least the required size\n",
    "        pad_d = max(0, sd - D)\n",
    "        pad_h = max(0, sh - H)\n",
    "        pad_w = max(0, sw - W)\n",
    "        \n",
    "        if pad_d > 0 or pad_h > 0 or pad_w > 0:\n",
    "            img = F.pad(img, (0, pad_w, 0, pad_h, 0, pad_d))\n",
    "            lbl = F.pad(lbl, (0, pad_w, 0, pad_h, 0, pad_d))\n",
    "            # Update dimensions after padding\n",
    "            _, D, H, W = img.shape\n",
    "        \n",
    "        # Now crop to exact size\n",
    "        d1 = random.randint(0, max(0, D - sd))\n",
    "        h1 = random.randint(0, max(0, H - sh))\n",
    "        w1 = random.randint(0, max(0, W - sw))\n",
    "        \n",
    "        img_crop = img[:, d1:d1+sd, h1:h1+sh, w1:w1+sw]\n",
    "        lbl_crop = lbl[:, d1:d1+sd, h1:h1+sh, w1:w1+sw]\n",
    "        \n",
    "        return img_crop, lbl_crop\n",
    "\n",
    "    @staticmethod\n",
    "    def random_flip(img: torch.Tensor, lbl: torch.Tensor):\n",
    "        for axis in range(1, 4):\n",
    "            if random.random() > 0.5:\n",
    "                img = torch.flip(img, [axis])\n",
    "                lbl = torch.flip(lbl, [axis])\n",
    "        return img, lbl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebe7e54",
   "metadata": {},
   "source": [
    "## U-Net 3D Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "131624af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv3d(in_channels, out_channels, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm3d(out_channels)\n",
    "        self.conv2 = nn.Conv3d(out_channels, out_channels, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm3d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class UNet3D(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=1):\n",
    "        super().__init__()\n",
    "        self.down1 = ConvBlock(in_channels, 32)\n",
    "        self.down2 = ConvBlock(32, 64)\n",
    "        self.down3 = ConvBlock(64, 128)\n",
    "        self.down4 = ConvBlock(128, 256)\n",
    "\n",
    "        self.center = ConvBlock(256, 512)\n",
    "\n",
    "        self.up4 = nn.ConvTranspose3d(512, 256, 2, stride=2)\n",
    "        self.upconv4 = ConvBlock(512, 256)\n",
    "        self.up3 = nn.ConvTranspose3d(256, 128, 2, stride=2)\n",
    "        self.upconv3 = ConvBlock(256, 128)\n",
    "        self.up2 = nn.ConvTranspose3d(128, 64, 2, stride=2)\n",
    "        self.upconv2 = ConvBlock(128, 64)\n",
    "        self.up1 = nn.ConvTranspose3d(64, 32, 2, stride=2)\n",
    "        self.upconv1 = ConvBlock(64, 32)\n",
    "\n",
    "        self.out = nn.Conv3d(32, out_channels, 1)\n",
    "        self.pool = nn.MaxPool3d(2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.down1(x)\n",
    "        x2 = self.down2(self.pool(x1))\n",
    "        x3 = self.down3(self.pool(x2))\n",
    "        x4 = self.down4(self.pool(x3))\n",
    "\n",
    "        center = self.center(self.pool(x4))\n",
    "\n",
    "        up = self.up4(center)\n",
    "        up = torch.cat([up, x4], dim=1)\n",
    "        up = self.upconv4(up)\n",
    "\n",
    "        up = self.up3(up)\n",
    "        up = torch.cat([up, x3], dim=1)\n",
    "        up = self.upconv3(up)\n",
    "\n",
    "        up = self.up2(up)\n",
    "        up = torch.cat([up, x2], dim=1)\n",
    "        up = self.upconv2(up)\n",
    "\n",
    "        up = self.up1(up)\n",
    "        up = torch.cat([up, x1], dim=1)\n",
    "        up = self.upconv1(up)\n",
    "\n",
    "        return self.out(up)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de32dd53",
   "metadata": {},
   "source": [
    "## Loss Function and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b32289f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceBCELoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        inputs = torch.sigmoid(inputs)\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        intersection = (inputs * targets).sum()\n",
    "        dice = (2.*intersection + smooth) / (inputs.sum() + targets.sum() + smooth)\n",
    "        BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n",
    "        Dice_BCE = BCE + (1 - dice)\n",
    "        return Dice_BCE\n",
    "\n",
    "\n",
    "def dice_coefficient(pred, target, smooth=1e-6):\n",
    "    pred = torch.sigmoid(pred)\n",
    "    pred = (pred > 0.5).float()\n",
    "    intersection = (pred * target).sum()\n",
    "    return (2. * intersection + smooth) / (pred.sum() + target.sum() + smooth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f70689",
   "metadata": {},
   "source": [
    "## Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "415d0f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for imgs, lbls in loader:\n",
    "        imgs, lbls = imgs.to(device), lbls.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(imgs)\n",
    "        loss = loss_fn(logits, lbls)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    dices = []\n",
    "    with torch.no_grad():\n",
    "        for imgs, lbls in loader:\n",
    "            imgs, lbls = imgs.to(device), lbls.to(device)\n",
    "            logits = model(imgs)\n",
    "            dices.append(dice_coefficient(logits, lbls))\n",
    "    return float(np.mean(dices))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea7181c",
   "metadata": {},
   "source": [
    "## Data Loading Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f01c8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 60 image/label pairs\n",
      "First pair: im0.nii.gz <-> lb0.nii.gz\n"
     ]
    }
   ],
   "source": [
    "def find_pairs(images_dir, labels_dir):\n",
    "    imgs = sorted(glob(os.path.join(images_dir, \"im*.nii*\")))\n",
    "    lbls = sorted(glob(os.path.join(labels_dir, \"lb*.nii*\")))\n",
    "    \n",
    "    # Extract base numbers correctly\n",
    "    base_to_img = {}\n",
    "    for p in imgs:\n",
    "        basename = os.path.basename(p)\n",
    "        # Extract number from \"im{number}.nii.gz\"\n",
    "        base_num = basename.replace('im', '').replace('.nii.gz', '').replace('.nii', '')\n",
    "        base_to_img[base_num] = p\n",
    "    \n",
    "    base_to_lbl = {}\n",
    "    for p in lbls:\n",
    "        basename = os.path.basename(p)\n",
    "        # Extract number from \"lb{number}.nii.gz\"\n",
    "        base_num = basename.replace('lb', '').replace('.nii.gz', '').replace('.nii', '')\n",
    "        base_to_lbl[base_num] = p\n",
    "    \n",
    "    common = set(base_to_img) & set(base_to_lbl)\n",
    "    return [base_to_img[b] for b in sorted(common, key=int)], [base_to_lbl[b] for b in sorted(common, key=int)]\n",
    "\n",
    "# Test data loading\n",
    "images, labels = find_pairs(IMAGES_DIR, LABELS_DIR)\n",
    "print(f\"Found {len(images)} image/label pairs\")\n",
    "if len(images) > 0:\n",
    "    print(f\"First pair: {os.path.basename(images[0])} <-> {os.path.basename(labels[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e529c84",
   "metadata": {},
   "source": [
    "## Setup Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de4014be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 48, Validation samples: 12\n",
      "Using device: cpu\n",
      "Model parameters: 22,581,217\n"
     ]
    }
   ],
   "source": [
    "# Split data\n",
    "if not images:\n",
    "    raise RuntimeError('No image/label pairs found.')\n",
    "\n",
    "idx = list(range(len(images)))\n",
    "random.shuffle(idx)\n",
    "split = int(len(idx)*(1-VAL_SPLIT))\n",
    "train_idx, val_idx = idx[:split], idx[split:]\n",
    "\n",
    "print(f\"Training samples: {len(train_idx)}, Validation samples: {len(val_idx)}\")\n",
    "\n",
    "# Create datasets\n",
    "train_ds = NiftiSegDataset([images[i] for i in train_idx], [labels[i] for i in train_idx], patch_size=PATCH_SIZE)\n",
    "val_ds = NiftiSegDataset([images[i] for i in val_idx], [labels[i] for i in val_idx], patch_size=PATCH_SIZE, augment=False)\n",
    "\n",
    "# Create data loaders (reduced workers for compatibility)\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=False)\n",
    "val_loader = DataLoader(val_ds, batch_size=1, shuffle=False, num_workers=0, pin_memory=False)\n",
    "\n",
    "# Setup device and model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "\n",
    "model = UNet3D().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "loss_fn = DiceBCELoss()\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2192e328",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678c1a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "best_val_dice = 0\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    print(f\"\\n=== Epoch {epoch}/{EPOCHS} ===\")\n",
    "    \n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, loss_fn, device)\n",
    "    val_dice = evaluate(model, val_loader, device)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f}, Val Dice: {val_dice:.4f}\")\n",
    "    \n",
    "    if val_dice > best_val_dice:\n",
    "        best_val_dice = val_dice\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state': model.state_dict(),\n",
    "            'optimizer_state': optimizer.state_dict(),\n",
    "            'val_dice': val_dice\n",
    "        }, SAVE_PATH)\n",
    "        print(f\"âœ“ Saved best model (val_dice={val_dice:.4f}) to {SAVE_PATH}\")\n",
    "\n",
    "print(f\"\\nðŸŽ‰ Training complete! Best validation Dice: {best_val_dice:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
